{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import (\n",
    "    Dirichlet, Categorical, Empirical, ParamMixture)\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mimic_data(data_dir):\n",
    "    \"\"\"Loads a dataset extracted from the MIMIC-III critical care database.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        w - List of lists of 1D NumPy arrays containing\n",
    "            tokens for each individual and data source.\n",
    "        dicts - List of token id to token dictionaries for \n",
    "                all data types\n",
    "    \"\"\"\n",
    "    \n",
    "    data_types = ['note', 'lab', 'med']\n",
    "\n",
    "    D = 10  # number of patients    \n",
    "    S = len(glob.glob(os.path.join(data_dir, 'corpora/*.txt')))\n",
    "\n",
    "    dicts = [None] * S\n",
    "    w = [[None] * S for d in range(D)]\n",
    "    z = [[None] * S for d in range(D)]\n",
    "    for s, dt in enumerate(data_types):\n",
    "        dict_file = os.path.join(data_dir,\n",
    "                                 'dicts',\n",
    "                                 dt + '_dict.p')\n",
    "\n",
    "        form_corpus_file = os.path.join(data_dir, \n",
    "                                        'form_corpora',\n",
    "                                        dt + '_form_corpus.txt')\n",
    "        \n",
    "        with open(dict_file, 'rb') as file:\n",
    "            dicts[s] = pickle.load(file)\n",
    "            \n",
    "\n",
    "        with open(form_corpus_file, 'r') as file:\n",
    "            for d, line in enumerate(file):\n",
    "                doc_tokenids = []\n",
    "                tokenid_counts = line.split(' ')[1:]\n",
    "\n",
    "                for tic in tokenid_counts:\n",
    "                    ti_c = tic.strip().split(':')\n",
    "                    tokenid = float(ti_c[0])\n",
    "                    count = int(ti_c[1])\n",
    "                    \n",
    "                    if count == 1:\n",
    "                        count += 1\n",
    "\n",
    "                    for _ in range(count):\n",
    "                        doc_tokenids.append(tokenid)\n",
    "\n",
    "                w[d][s] = np.array(doc_tokenids)\n",
    "    \n",
    "    return w, dicts, D, S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# DATA\n",
    "###############\n",
    "\n",
    "# Get MIMIC data\n",
    "w_train, dicts, D, S = load_mimic_data('../data')\n",
    "\n",
    "# Calculate vocabulary size for each data type\n",
    "V = [None] * S\n",
    "for s in range(S):\n",
    "    V[s] = len(dicts[s])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# MODEL\n",
    "###############\n",
    "K = 5\n",
    "\n",
    "alpha = tf.ones(K) * 0.01\n",
    "\n",
    "beta, phi = [None] * S, [None] * S\n",
    "for s in range(S):\n",
    "    beta[s] = tf.ones(V[s]) * 0.01\n",
    "    phi[s] = Dirichlet(concentration=beta[s], \n",
    "                       sample_shape=K)\n",
    "\n",
    "theta = [None] * D\n",
    "w = [[None] * S for d in range(D)]\n",
    "z = [[None] * S for d in range(D)]\n",
    "for d in range(D):\n",
    "    theta[d] = Dirichlet(concentration=alpha)\n",
    "    \n",
    "    for s in range(S):\n",
    "\n",
    "        w[d][s] = ParamMixture(mixing_weights=theta[d], \n",
    "                               component_params={'probs': phi[s]},\n",
    "                               component_dist=Categorical,\n",
    "                               sample_shape=len(w_train[d][s]),\n",
    "                               validate_args=True)\n",
    "\n",
    "        z[d][s] = w[d][s].cat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "####################\n",
    "#INFERENCE\n",
    "####################\n",
    "\n",
    "overall_time = time.time()\n",
    "\n",
    "# Data vars\n",
    "data_dict = {}\n",
    "for d in range(D):\n",
    "    for s in range(S):\n",
    "        data_dict[w[d][s]] = w_train[d][s]\n",
    "\n",
    "\n",
    "# Latent vars\n",
    "latent_vars_dict = {}\n",
    "\n",
    "T = 5000 # number of samples\n",
    "qphi = [None] * S\n",
    "for s in range(S):\n",
    "    print('Building latents for phi {} of {}'.format(s + 1, S))\n",
    "    qphi[s] = Empirical(tf.Variable(tf.zeros([T, K, V[s]])))\n",
    "    latent_vars_dict[phi[s]] = qphi[s]\n",
    "\n",
    "qtheta = [None] * D\n",
    "qz = [[None] * S for d in range(D)]\n",
    "for d in range(D):\n",
    "    print('Building latents for doc {} of {}'.format(d + 1, D))\n",
    "    qtheta[d]= Empirical(tf.Variable(tf.ones([T, K]) / K))\n",
    "    latent_vars_dict[theta[d]] = qtheta[d]\n",
    "    \n",
    "    for s in range(S):\n",
    "        N = len(w_train[d][s])\n",
    "\n",
    "        qz[d][s] = Empirical(tf.Variable(tf.zeros([T, N], dtype=tf.int32)))\n",
    "        latent_vars_dict[z[d][s]] = qz[d][s]\n",
    "print()\n",
    "\n",
    "# Proposal vars\n",
    "proposal_vars_dict = {}\n",
    "\n",
    "phi_cond = [None] * S\n",
    "for s in range(S):\n",
    "    iteration_time = time.time()\n",
    "    print('Building proposals for phi {} of {}'.format(s + 1, S))\n",
    "    phi_cond[s] = ed.complete_conditional(phi[s])\n",
    "    proposal_vars_dict[phi[s]] = phi_cond[s]\n",
    "    end = time.time()\n",
    "    print('Overall time: {}, Iteration time: {}'.format(end - overall_time,\n",
    "                                                        end - iteration_time))\n",
    "\n",
    "          \n",
    "theta_cond = [None] * D\n",
    "z_cond = [[None] * S for d in range(D)]\n",
    "for d in range(D):\n",
    "    iteration_time = time.time()\n",
    "    print('Building proposals for doc {} of {}'.format(d + 1, D))\n",
    "          \n",
    "    theta_cond[d] = ed.complete_conditional(theta[d])\n",
    "    proposal_vars_dict[theta[d]] = theta_cond[d]\n",
    "    \n",
    "    for s in range(S):\n",
    "        z_cond[d][s] = ed.complete_conditional(z[d][s])\n",
    "        proposal_vars_dict[z[d][s]] = z_cond[d][s]\n",
    "        \n",
    "    end = time.time()\n",
    "    print('Overall time: {}, Iteration time: {}'.format(end - overall_time,\n",
    "                                                        end - iteration_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference procedure w/Gibbs sampling\n",
    "inference = ed.Gibbs(latent_vars=latent_vars_dict,\n",
    "                     proposal_vars=proposal_vars_dict,\n",
    "                     data=data_dict)\n",
    "\n",
    "inference.initialize(n_iter=T, n_print=10, logdir='log')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for n in range(inference.n_iter):\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)\n",
    "    \n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def gen_wordcloud(tokens, probs):\n",
    "\n",
    "    def red_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(0, 100%%, %d%%)\" % random.randint(25, 55)\n",
    "\n",
    "    def green_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(90, 100%%, %d%%)\" % random.randint(27, 37)\n",
    "\n",
    "    def blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(200, 100%%, %d%%)\" % random.randint(30, 60)\n",
    "\n",
    "\n",
    "    data_types = ['note', 'lab', 'med']\n",
    "\n",
    "    colors = {0 : red_color_func, \n",
    "              1 : green_color_func,\n",
    "              2 : blue_color_func}\n",
    "\n",
    "    f, axn = plt.subplots(1, 3)\n",
    "\n",
    "    for j, ax in enumerate(axn.flatten()):\n",
    "\n",
    "        tokens_probs = dict((t, p) for t, p in zip(tokens[j], probs[j]))\n",
    "\n",
    "        wordcloud = WordCloud(background_color=\"white\", relative_scaling=0.5)\n",
    "        wordcloud.generate_from_frequencies(tokens_probs)\n",
    "        wordcloud.recolor(color_func=colors[j], random_state=3)\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.imshow(wordcloud)        \n",
    "        ax.set_title(data_types[j], fontsize=50)\n",
    "\n",
    "        #plt.savefig('../figures/wcloud_phen{}.pdf'.format(phen_num), dpi=1000, format='pdf', pad_inches=0)\n",
    "    f = plt.gcf()\n",
    "    f.set_figwidth(36.0)\n",
    "    f.set_figheight(36.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qphi_sample = [None] * S\n",
    "for s in range(S):\n",
    "    qphi_sample[s] = qphi[s].params[-1].eval()\n",
    "    \n",
    "probs = [[None] * S for k in range(K)]\n",
    "for k in range(K):\n",
    "    for s in range(S):\n",
    "        probs[k][s] = qphi_sample[s][k, :]\n",
    "        \n",
    "tokens = [[None] * V[s] for s in range(S)]\n",
    "for s in range(S):\n",
    "    for i, tok in enumerate(dicts[s].values()):\n",
    "        tokens[s][i] = tok\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    gen_wordcloud(tokens, probs[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(qphi[1].params[-1].eval())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = tf.pow(qphi[1] - tf.reduce_mean(qphi[1].params, axis=0), 2)\n",
    "plt.pcolormesh(np.log(var.eval()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
