{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import (\n",
    "    Dirichlet, Categorical, Empirical, ParamMixture)\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(D, V, K):\n",
    "    \"\"\"\n",
    "    Builds a toy dataset of D documents.\n",
    "    \n",
    "    Args:\n",
    "        D - Number of documents\n",
    "        V - Size of vocabulary\n",
    "        K - Number of topics\n",
    "        \n",
    "    Returns:\n",
    "        w - List of 1D NumPy arrays containing\n",
    "            word tokens for each document\n",
    "        theta - 2D NumPy array containing document\n",
    "                topic distributions\n",
    "        phi - 2D NumPy array containing topic\n",
    "               token distributions\n",
    "    \"\"\"\n",
    "    # Draw number of tokens for each document\n",
    "    N = np.random.randint(low=2,\n",
    "                          high=20,\n",
    "                          size=D)\n",
    "    \n",
    "    # Draw topic distributions for each document\n",
    "    theta = np.random.dirichlet(alpha=np.ones(K) * 0.1, \n",
    "                                size=D)\n",
    "\n",
    "    # Create topics (non-overlapping)\n",
    "    phi_values = np.array([K * 1 / V] * int(V / K) + [0.0] * int(V - V / K))\n",
    "    phi = np.zeros([K, V])\n",
    "    for k in range(K):\n",
    "        phi[k, :] = np.roll(phi_values, int(k * V / K))\n",
    "    \n",
    "    # Draw tokens for each document\n",
    "    w, z = [None] * D, [None] * D\n",
    "    for d in range(D):\n",
    "        # Draw token topic assignments\n",
    "        z[d] = np.array([np.random.choice(range(K), size=N[d], p=theta[d, :])])[0]\n",
    "        # Draw tokens\n",
    "        w[d] = np.zeros(N[d])\n",
    "        for n in range(N[d]):\n",
    "            w[d][n] = np.random.choice(range(V), size=1, p=phi[z[d][n], :]) \n",
    "\n",
    "    return N, w, z, theta, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# DATA\n",
    "###############\n",
    "\n",
    "# Get toy dataset\n",
    "D = 10\n",
    "V = 30\n",
    "K = 5\n",
    "N_train, w_train, z_train, theta_train, phi_train = build_toy_dataset(D, V, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# MODEL\n",
    "###############\n",
    "\n",
    "alpha = tf.ones(K) * 0.01\n",
    "beta = tf.ones(V) * 0.01\n",
    "\n",
    "phi = Dirichlet(concentration=beta, \n",
    "                 sample_shape=K)\n",
    "\n",
    "theta, w, z = [None] * D, [None] * D, [None] * D\n",
    "for d in range(D):\n",
    "    theta[d] = Dirichlet(concentration=alpha)\n",
    "    \n",
    "    w[d] = ParamMixture(mixing_weights=theta[d], \n",
    "                        component_params={'probs': phi},\n",
    "                        component_dist=Categorical,\n",
    "                        sample_shape=len(w_train[d]),\n",
    "                        validate_args=True)\n",
    "    z[d] = w[d].cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building proposals for doc 1 of 10\n",
      "Building proposals for doc 2 of 10\n",
      "Building proposals for doc 3 of 10\n",
      "Building proposals for doc 4 of 10\n",
      "Building proposals for doc 5 of 10\n",
      "Building proposals for doc 6 of 10\n",
      "Building proposals for doc 7 of 10\n",
      "Building proposals for doc 8 of 10\n",
      "Building proposals for doc 9 of 10\n",
      "Building proposals for doc 10 of 10\n",
      "500/500 [100%] ██████████████████████████████ Elapsed: 203s | Acceptance Rate: 1.000\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "#INFERENCE\n",
    "####################\n",
    "\n",
    "# Data vars\n",
    "data_dict={}\n",
    "for d in range(D):\n",
    "    data_dict[w[d]] = w_train[d]\n",
    "\n",
    "# Latent vars\n",
    "latent_vars_dict = {}\n",
    "\n",
    "T = 500\n",
    "qphi = Empirical(tf.Variable(tf.zeros([T, K, V])))\n",
    "latent_vars_dict[phi] = qphi\n",
    "\n",
    "qz, qtheta = [None] * D, [None] * D\n",
    "for d in range(D):\n",
    "    qtheta[d] = Empirical(tf.Variable(tf.ones([T, K]) / K))\n",
    "    latent_vars_dict[theta[d]] = qtheta[d]\n",
    "    \n",
    "    N = len(w_train[d])\n",
    "    \n",
    "    qz[d] = Empirical(tf.Variable(tf.zeros([T, N], dtype=tf.int32)))\n",
    "    latent_vars_dict[z[d]] = qz[d]\n",
    "    \n",
    "\n",
    "# Proposal vars\n",
    "proposal_vars_dict = {}\n",
    "\n",
    "phi_cond = ed.complete_conditional(phi)\n",
    "proposal_vars_dict[phi] = phi_cond\n",
    "\n",
    "theta_cond, z_cond = [None] * D, [None] * D\n",
    "for d in range(D):\n",
    "    print('Building proposals for doc {} of {}'.format(d + 1, D))\n",
    "          \n",
    "    theta_cond[d] = ed.complete_conditional(theta[d])\n",
    "    proposal_vars_dict[theta[d]] = theta_cond[d]\n",
    "    \n",
    "    z_cond[d] = ed.complete_conditional(z[d])\n",
    "    proposal_vars_dict[z[d]] = z_cond[d]\n",
    "    \n",
    "\n",
    "# Inference procedure w/Gibbs sampling\n",
    "inference = ed.Gibbs(latent_vars=latent_vars_dict,\n",
    "                     proposal_vars=proposal_vars_dict,\n",
    "                     data=data_dict)\n",
    "\n",
    "inference.initialize(n_iter=T, n_print=10, logdir='log')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for n in range(inference.n_iter):\n",
    "    info_dict = inference.update()\n",
    "    inference.print_progress(info_dict)\n",
    "    \n",
    "inference.finalize()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
